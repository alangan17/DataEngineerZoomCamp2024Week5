version: '3'
services:
  mage:
    # Service for Data Transformation using shared MS SQL Server
    # Container will not spin up unless `docker compose --profile t up` is used
    build:
      context: devops/mage
      dockerfile: Dockerfile
      args:
        PIPENVFILE: requirements.txt
    image: ${IMAGE_REGISTRY}/${IMAGE_REPO}/mage:latest
    env_file:
      - .env
    volumes:
      ## Mount docker host directory to the container (i.e. current directory)
      - .:/home/src
    ports:
      - "127.0.0.1:6789-6799:6789" # mage
      - "127.0.0.1:8082-8090:8080" # dbt docs
    networks:
      - magic-network
      # deploy:
      # resources:
      # limits:
      # memory: 1000M

  minio:
    ## Use `docker compose --profile storage up -d` to spin up this container
    ## Local Object Storage
    ## Ref: https://min.io/docs/minio/container/index.html
    profiles:
      - storage
    image: quay.io/minio/minio
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_USER}
    volumes:
      - ./data:/data
    networks:
      - magic-network
    deploy:
      resources:
        limits:
          memory: 1000M
    restart: on-failure:5
    ## Python Usage:
    ## Ref: https://github.com/minio/minio-py?tab=readme-ov-file#example---file-uploader

    ## DuckDB Usage:
    ## Ref: https://blog.min.io/duckdb-and-minio-for-a-modern-data-stack/
    ## bash:
    ## ```
    ##   duckdb
    ##   INSTALL httpfs;
    ##   LOAD httpfs;
    ##   SET s3_endpoint='minio:9000';
    ##   SET s3_use_ssl=0;
    ##   SET s3_access_key_id='<MINIO_ROOT_USER>';
    ##   SET s3_secret_access_key='<MINIO_ROOT_PASSWORD>';

    ##   CREATE TABLE cost_tbl AS SELECT * FROM read_csv_auto('s3://01-raw/cost-2023-11-01-2023-12-01.csv', all_varchar=1);
    ##   CREATE TABLE cost_tbl AS SELECT * FROM read_csv_auto('http://minio:9000/01-raw/cost-2023-11-01-2023-12-01.csv', all_varchar=1);
    ##   SELECT * FROM cost_tbl;
    ## ```

  spark:
    # ref: https://gist.github.com/rafik-rahoui/f98df941c4ccced9c46e9ccbdef63a03
    # Use `docker compose --profile spark up -d` to spin up this container
    profiles:
      - spark
    build:
      context: devops/spark
      dockerfile: ./Dockerfile
      args:
        PIPENVFILE: requirements.txt
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '127.0.0.1:8080:8080'
      - '127.0.0.1:8888:8888' # this port is for jupyter notebook
      - '127.0.0.1:4040-4050:4040' # this port is for spark UI, you may need to open 4041 or 4042 in case 4040 is occupied  
    networks:
      - magic-network
    volumes:
      - "./:/opt/spark:rw"

  spark-worker:
    profiles:
      - spark
    image: docker.io/bitnami/spark:3.3
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - '127.0.0.1:8081:8081' # this port is usde to acces the worker UI
    networks:
      - magic-network

networks:
  magic-network:
    driver: bridge
